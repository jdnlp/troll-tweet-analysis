{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk0.csv sentiment analysis results: \n",
      "Polarity of training data: 0.3125\n",
      "Subjectivity of training data: 0.5125\n"
     ]
    }
   ],
   "source": [
    "# Name: James\n",
    "# Kennesaw State University\n",
    "# Professor: - removed for privacy reasons\n",
    "# Class: Natural Language Processing Section 01\n",
    "# Final Project\n",
    "\n",
    "from spacy.lang.en import English, STOP_WORDS\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "nlp.add_pipe('spacytextblob')\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "# The below commented code is what I used to create the 21 different csv files that I have, all besides the last containing ~10000 tweets\n",
    "# They are chunks of a corpus that I have linked to in the final report\n",
    "\n",
    "# size = 10000\n",
    "# filename = 'User_Tweets.csv'\n",
    "# for i, chunk in enumerate(pd.read_csv(filename, chunksize = size, encoding = 'utf-8', encoding_errors = 'replace')):\n",
    "    #chunk.to_csv(f\"chunk{i}.csv\", index = False)\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# ----------------------------------- Deprecated, only keeping because I spent a lot of time on this ---------------------------------\n",
    "# Figuring out how to make the line below work took me a while. \n",
    "# The csv file type encoding was having issues when I tried to read it due to not having a way to handle the emojis and other strange characters that are present in tweets\n",
    "# nrows is an int parameter that determines how many rows of the csv file will be accessed. 10000 is the number chosen for this project.\n",
    "# Making a new doc would simply require the int skiprows parameter to be added to the argument of read_csv, and set to the value 10000 to get another slice of the corpus\n",
    "\n",
    "# train_tweets = pd.read_csv('User_Tweets.csv', nrows = 10000, encoding = 'utf-8', encoding_errors = 'replace')\n",
    "\n",
    "# The data set I am using is a csv file with two columns: One called \"user_key\" and another called \"tweet_text_only\"\n",
    "# The column \"tweet_text_only\" has already been cleaned a bit\n",
    "# Isolate the column that contains the tweets: all I care about for this project since I am interested in sentiment analysis and not much else.\n",
    "\n",
    "# chunk0.csv analysis\n",
    "tweets0 = pd.read_csv('chunk0.csv')\n",
    "tweets0_text = tweets0.tweet_text_only\n",
    "\n",
    "#combine rows into long string prior to tokenization, because the nlp() function ONLY takes in a string as an argument\n",
    "\n",
    "tweets0_text_combined = str(tweets0_text)\n",
    "\n",
    "tweets0_text_combined.split()\n",
    "\n",
    "#imported regex will allow me to clean up the VERY messy tweets... only interested in letters\n",
    "\n",
    "clean_tweets0_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets0_text_combined)\n",
    "\n",
    "# remove whitespace before and after tweets in the document\n",
    "\n",
    "clean_tweets0_text = clean_tweets0_text.strip()\n",
    "\n",
    "#make document of the combined text\n",
    "\n",
    "tweets0_doc = nlp(clean_tweets0_text)\n",
    "\n",
    "# Data from chunk0.csv: first 10000 tweets in corpus\n",
    "# Polarity is a value between negative (-1.0), neutral (0) and positive (1.0)\n",
    "# Subjectivity is a value between 0 and 1, where 0 is not at all subjective, and 1 is very subjective\n",
    "print('Chunk0.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets0_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets0_doc._.blob.subjectivity))\n",
    "\n",
    "\n",
    "# This method of creating and cleaning/preprocessing documents, then running sentiment analysis on them will be replicated for the other documents in the set in the following code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk1.csv sentiment analysis results: \n",
      "Polarity: 0.10625\n",
      "Subjectivity: 0.49374999999999997\n"
     ]
    }
   ],
   "source": [
    "# chunk1.csv analysis\n",
    "\n",
    "tweets1 = pd.read_csv('chunk1.csv')\n",
    "\n",
    "tweets1_text = tweets1.tweet_text_only\n",
    "\n",
    "tweets1_text_combined = str(tweets1_text)\n",
    "\n",
    "tweets1_text_combined.split()\n",
    "\n",
    "clean_tweets1_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets1_text_combined)\n",
    "\n",
    "clean_tweets1_text = clean_tweets1_text.strip()\n",
    "\n",
    "tweets1_doc = nlp(clean_tweets1_text)\n",
    "\n",
    "print('Chunk1.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets1_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets1_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk2.csv sentiment analysis results: \n",
      "Polarity: -0.13194444444444445\n",
      "Subjectivity: 0.673611111111111\n"
     ]
    }
   ],
   "source": [
    "# chunk2.csv analysis\n",
    "\n",
    "tweets2 = pd.read_csv('chunk2.csv')\n",
    "\n",
    "tweets2_text = tweets2.tweet_text_only\n",
    "\n",
    "tweets2_text_combined = str(tweets2_text)\n",
    "\n",
    "tweets2_text_combined.split()\n",
    "\n",
    "clean_tweets2_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets2_text_combined)\n",
    "\n",
    "clean_tweets2_text = clean_tweets2_text.strip()\n",
    "\n",
    "tweets2_doc = nlp(clean_tweets2_text)\n",
    "\n",
    "print('Chunk2.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets2_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets2_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk3.csv sentiment analysis results: \n",
      "Polarity of training data: 0.3333333333333333\n",
      "Subjectivity of training data: 0.4666666666666666\n"
     ]
    }
   ],
   "source": [
    "# chunk3.csv analysis\n",
    "\n",
    "tweets3 = pd.read_csv('chunk3.csv')\n",
    "tweets3_text = tweets3.tweet_text_only\n",
    "\n",
    "tweets3_text_combined = str(tweets3_text)\n",
    "\n",
    "tweets3_text_combined.split()\n",
    "\n",
    "clean_tweets3_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets3_text_combined)\n",
    "\n",
    "clean_tweets3_text = clean_tweets3_text.strip()\n",
    "\n",
    "tweets3_doc = nlp(clean_tweets3_text)\n",
    "\n",
    "print('Chunk3.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets3_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets3_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk4.csv sentiment analysis results: \n",
      "Polarity of training data: 0.16380102040816327\n",
      "Subjectivity of training data: 0.6638775510204081\n"
     ]
    }
   ],
   "source": [
    "# chunk4.csv analysis\n",
    "\n",
    "tweets4 = pd.read_csv('chunk4.csv')\n",
    "\n",
    "tweets4_text = tweets4.tweet_text_only\n",
    "\n",
    "tweets4_text_combined = str(tweets4_text)\n",
    "\n",
    "tweets4_text_combined.split()\n",
    "\n",
    "clean_tweets4_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets4_text_combined)\n",
    "\n",
    "clean_tweets4_text = clean_tweets4_text.strip()\n",
    "\n",
    "tweets4_doc = nlp(clean_tweets4_text)\n",
    "\n",
    "print('Chunk4.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets4_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets4_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk5.csv sentiment analysis results: \n",
      "Polarity: 0.25\n",
      "Subjectivity: 0.375\n"
     ]
    }
   ],
   "source": [
    "# chunk5.csv analysis\n",
    "\n",
    "tweets5 = pd.read_csv('chunk5.csv')\n",
    "\n",
    "tweets5_text = tweets5.tweet_text_only\n",
    "\n",
    "tweets5_text_combined = str(tweets5_text)\n",
    "\n",
    "tweets5_text_combined.split()\n",
    "\n",
    "clean_tweets5_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets5_text_combined)\n",
    "\n",
    "clean_tweets5_text = clean_tweets5_text.strip()\n",
    "\n",
    "tweets5_doc = nlp(clean_tweets5_text)\n",
    "\n",
    "print('Chunk5.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets5_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets5_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk6.csv sentiment analysis results: \n",
      "Polarity: 0.14054112554112555\n",
      "Subjectivity: 0.5095021645021645\n"
     ]
    }
   ],
   "source": [
    "# chunk6.csv analysis\n",
    "\n",
    "tweets6 = pd.read_csv('chunk6.csv')\n",
    "\n",
    "tweets6_text = tweets6.tweet_text_only\n",
    "\n",
    "tweets6_text_combined = str(tweets6_text)\n",
    "\n",
    "tweets6_text_combined.split()\n",
    "\n",
    "clean_tweets6_text = re.sub(r\"[^a-zA-Z]\", \" \", tweets6_text_combined)\n",
    "\n",
    "clean_tweets6_text = clean_tweets6_text.strip()\n",
    "\n",
    "tweets6_doc = nlp(clean_tweets6_text)\n",
    "\n",
    "print('Chunk6.csv sentiment analysis results: ')\n",
    "print('Polarity: ' + str(tweets6_doc._.blob.polarity))\n",
    "print('Subjectivity: ' + str(tweets6_doc._.blob.subjectivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44191f51458e4e5bd0ec2b4faeff152eb364d38382f1e0f9a3a9b975cd168161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
